Hyperparameters explanation: 

The model uses a gradient descent batch size of 64 to balance computational efficiency and stability during training, with an entropy value of 0.01 to encourage moderate exploration and prevent premature convergence. A discount factor of 0.88 emphasizes near-term rewards while still valuing future rewards. The mean squared error loss function ensures accurate policy updates, while a learning rate of 0.0003 allows for stable convergence. The policy is updated every 18 episodes to gather sufficient experience, and 4 epochs are used to process collected data without overfitting. These hyperparameters, optimized for a PPO algorithm in a continuous action space, are designed to balance exploration and exploitation for effective learning in a time-trial racing environment.